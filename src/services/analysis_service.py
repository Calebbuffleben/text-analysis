"""
ServiÃ§o principal de anÃ¡lise de texto.
Orquestra anÃ¡lise com BERT, gerencia cache e agrega resultados.
"""

from typing import Dict, Any, Tuple, List
from ..types.messages import TranscriptionChunk
from ..models.bert_analyzer import BERTAnalyzer
from ..services.cache_service import AnalysisCache
from ..config import Config
import structlog
import time

logger = structlog.get_logger()


class TextAnalysisService:
    """
    ServiÃ§o de anÃ¡lise de texto com BERT.
    
    Responsabilidades:
    - Gerenciar cache de resultados
    - Lazy loading do analisador BERT
    - Orquestrar anÃ¡lise (sentimento, keywords, emoÃ§Ãµes)
    - Agregar resultados
    """
    
    def __init__(self):
        """Inicializa serviÃ§o de anÃ¡lise"""
        self.analyzer = None
        self.cache = AnalysisCache(
            ttl_seconds=Config.CACHE_TTL_SECONDS,
            max_size=Config.CACHE_MAX_SIZE
        )
        
        logger.info(
            "âœ… [SERVIÃ‡O] TextAnalysisService inicializado",
            cache_ttl=Config.CACHE_TTL_SECONDS,
            cache_max_size=Config.CACHE_MAX_SIZE
        )
    
    def _get_analyzer(self) -> BERTAnalyzer:
        """
        Retorna analisador BERT (lazy loading).
        
        Returns:
            InstÃ¢ncia de BERTAnalyzer
        """
        if self.analyzer is None:
            logger.info("Initializing BERT analyzer")
            self.analyzer = BERTAnalyzer(
                model_name=Config.MODEL_NAME,
                device=Config.MODEL_DEVICE,
                cache_dir=Config.MODEL_CACHE_DIR,
                max_length=Config.ANALYSIS_MAX_LENGTH,
                sbert_model_name=getattr(Config, 'SBERT_MODEL_NAME', None)
            )
        return self.analyzer
    
    async def analyze(self, chunk: TranscriptionChunk) -> Dict[str, Any]:
        """
        Analisa texto e retorna resultados completos.
        
        Fluxo:
        1. Verifica cache
        2. Se nÃ£o encontrado, executa anÃ¡lise
        3. Armazena no cache
        4. Retorna resultados
        
        Args:
            chunk: Chunk de transcriÃ§Ã£o a ser analisado
            
        Returns:
            Dict com resultados da anÃ¡lise:
            {
                'word_count': int,
                'char_count': int,
                'has_question': bool,
                'has_exclamation': bool,
                'sentiment_score': Dict[str, float],
                'emotions': Dict[str, float],
                'topics': List[str],
                'keywords': List[str]
            }
        """
        start_time = time.perf_counter()
        
        logger.debug(
            "ğŸ” [ANÃLISE] Verificando cache",
            meeting_id=chunk.meetingId,
            participant_id=chunk.participantId,
            text_length=len(chunk.text)
        )
        
        # Verificar cache primeiro
        cached_result = self.cache.get(
            chunk.meetingId,
            chunk.participantId,
            chunk.text
        )
        
        if cached_result:
            latency_ms = (time.perf_counter() - start_time) * 1000
            logger.info(
                "âœ… [ANÃLISE] Resultado encontrado no cache",
                meeting_id=chunk.meetingId,
                participant_id=chunk.participantId,
                latency_ms=round(latency_ms, 2)
            )
            return cached_result
        
        logger.info(
            "âš™ï¸ [ANÃLISE] Cache miss, executando anÃ¡lise completa",
            meeting_id=chunk.meetingId,
            participant_id=chunk.participantId,
            text_length=len(chunk.text),
            word_count=len(chunk.text.split())
        )
        
        # Obter analisador (lazy loading)
        analyzer = self._get_analyzer()
        
        # Executar anÃ¡lises em paralelo (futuro: usar asyncio.gather)
        logger.debug(
            "ğŸ“Š [ANÃLISE] Executando anÃ¡lise de sentimento",
            meeting_id=chunk.meetingId
        )
        sentiment = analyzer.analyze_sentiment(chunk.text)
        
        logger.debug(
            "ğŸ”‘ [ANÃLISE] Extraindo keywords",
            meeting_id=chunk.meetingId
        )
        keywords = analyzer.extract_keywords(chunk.text, top_n=10)
        
        logger.debug(
            "ğŸ˜Š [ANÃLISE] Detectando emoÃ§Ãµes",
            meeting_id=chunk.meetingId
        )
        emotions = analyzer.detect_emotions(chunk.text)
        
        # AnÃ¡lise semÃ¢ntica com SBERT
        # Esta anÃ¡lise gera embeddings semÃ¢nticos e pode calcular similaridade
        # com textos anteriores (Ãºtil para detectar repetiÃ§Ã£o de ideias)
        semantic_analysis = None
        try:
            if Config.SBERT_MODEL_NAME:
                logger.debug(
                    "ğŸ§  [ANÃLISE] Executando anÃ¡lise semÃ¢ntica com SBERT",
                    meeting_id=chunk.meetingId
                )
                # Realizar anÃ¡lise semÃ¢ntica completa
                # Por enquanto, nÃ£o passamos textos de referÃªncia, mas isso pode ser
                # implementado no futuro para detectar repetiÃ§Ã£o de ideias
                semantic_analysis = analyzer.analyze_semantics(chunk.text)
                logger.debug(
                    "âœ… [ANÃLISE] AnÃ¡lise semÃ¢ntica concluÃ­da",
                    meeting_id=chunk.meetingId,
                    embedding_dim=semantic_analysis.get('embedding_dimension', 0)
                )
        except Exception as e:
            # Se a anÃ¡lise semÃ¢ntica falhar, continuar sem ela
            logger.warn(
                "âš ï¸ [ANÃLISE] AnÃ¡lise semÃ¢ntica falhou, continuando sem ela",
                error=str(e),
                meeting_id=chunk.meetingId
            )
        
        # Calcular mÃ©tricas bÃ¡sicas
        word_count = len(chunk.text.split())
        char_count = len(chunk.text)
        has_question = '?' in chunk.text
        has_exclamation = '!' in chunk.text
        
        logger.debug(
            "ğŸ“ [ANÃLISE] MÃ©tricas bÃ¡sicas calculadas",
            meeting_id=chunk.meetingId,
            word_count=word_count,
            char_count=char_count,
            has_question=has_question,
            has_exclamation=has_exclamation
        )
        
        # Determinar sentimento como string (maior score)
        sentiment_label = 'neutral'
        sentiment_single_score = sentiment.get('neutral', 0.0)
        if sentiment.get('positive', 0.0) > sentiment.get('negative', 0.0) and sentiment.get('positive', 0.0) > sentiment.get('neutral', 0.0):
            sentiment_label = 'positive'
            sentiment_single_score = sentiment.get('positive', 0.0)
        elif sentiment.get('negative', 0.0) > sentiment.get('neutral', 0.0):
            sentiment_label = 'negative'
            sentiment_single_score = sentiment.get('negative', 0.0)
        
        logger.debug(
            "ğŸ’­ [ANÃLISE] Sentimento determinado",
            meeting_id=chunk.meetingId,
            sentiment=sentiment_label,
            score=round(sentiment_single_score, 3)
        )
        
        # Detectar intent (intenÃ§Ã£o) - implementaÃ§Ã£o bÃ¡sica
        logger.debug(
            "ğŸ¯ [ANÃLISE] Detectando intenÃ§Ã£o",
            meeting_id=chunk.meetingId
        )
        intent, intent_confidence = self._detect_intent(chunk.text, has_question)
        
        # Detectar topic (tÃ³pico) - implementaÃ§Ã£o bÃ¡sica
        logger.debug(
            "ğŸ“Œ [ANÃLISE] Detectando tÃ³pico",
            meeting_id=chunk.meetingId
        )
        topic, topic_confidence = self._detect_topic(chunk.text, keywords)
        
        # Detectar speech_act (ato de fala) - implementaÃ§Ã£o bÃ¡sica
        logger.debug(
            "ğŸ—£ï¸ [ANÃLISE] Detectando ato de fala",
            meeting_id=chunk.meetingId
        )
        speech_act, speech_act_confidence = self._detect_speech_act(chunk.text, has_question, has_exclamation)
        
        # Extrair entities (entidades) - implementaÃ§Ã£o bÃ¡sica
        logger.debug(
            "ğŸ·ï¸ [ANÃLISE] Extraindo entidades",
            meeting_id=chunk.meetingId
        )
        entities = self._extract_entities(chunk.text, keywords)
        
        # Calcular urgency (urgÃªncia) - implementaÃ§Ã£o bÃ¡sica
        logger.debug(
            "âš¡ [ANÃLISE] Calculando urgÃªncia",
            meeting_id=chunk.meetingId
        )
        urgency = self._calculate_urgency(sentiment_single_score, has_question, has_exclamation, emotions)
        
        # Obter embedding completo se disponÃ­vel
        embedding = []
        try:
            if Config.SBERT_MODEL_NAME:
                logger.debug(
                    "ğŸ”¢ [ANÃLISE] Gerando embedding semÃ¢ntico",
                    meeting_id=chunk.meetingId
                )
                # Gerar embedding completo usando SBERT
                embedding_array = analyzer.generate_semantic_embedding(chunk.text)
                # Converter numpy array para lista Python
                import numpy as np
                if isinstance(embedding_array, np.ndarray):
                    embedding = embedding_array.tolist()
                else:
                    embedding = list(embedding_array)
                logger.debug(
                    "âœ… [ANÃLISE] Embedding gerado",
                    meeting_id=chunk.meetingId,
                    embedding_dim=len(embedding)
                )
        except Exception as e:
            logger.warn(
                "âš ï¸ [ANÃLISE] Falha ao gerar embedding",
                error=str(e),
                meeting_id=chunk.meetingId
            )
            embedding = []
        
        # Construir resultado completo com nova estrutura
        result = {
            'intent': intent,
            'intent_confidence': intent_confidence,
            'topic': topic,
            'topic_confidence': topic_confidence,
            'speech_act': speech_act,
            'speech_act_confidence': speech_act_confidence,
            'keywords': keywords,
            'entities': entities,
            'sentiment': sentiment_label,
            'sentiment_score': sentiment_single_score,
            'urgency': urgency,
            'embedding': embedding
        }
        
        # Armazenar no cache
        logger.debug(
            "ğŸ’¾ [ANÃLISE] Armazenando resultado no cache",
            meeting_id=chunk.meetingId,
            participant_id=chunk.participantId
        )
        self.cache.set(
            chunk.meetingId,
            chunk.participantId,
            chunk.text,
            result
        )
        
        latency_ms = (time.perf_counter() - start_time) * 1000
        
        logger.info(
            "âœ… [ANÃLISE] AnÃ¡lise completa concluÃ­da",
            meeting_id=chunk.meetingId,
            participant_id=chunk.participantId,
            word_count=word_count,
            char_count=char_count,
            sentiment=sentiment_label,
            sentiment_score=round(sentiment_single_score, 3),
            intent=intent,
            intent_confidence=round(intent_confidence, 3),
            topic=topic,
            topic_confidence=round(topic_confidence, 3),
            speech_act=speech_act,
            speech_act_confidence=round(speech_act_confidence, 3),
            urgency=round(urgency, 3),
            keywords_count=len(keywords),
            entities_count=len(entities),
            embedding_dim=len(embedding),
            latency_ms=round(latency_ms, 2)
        )
        
        return result
    
    def _detect_intent(self, text: str, has_question: bool) -> Tuple[str, float]:
        """
        Detecta intenÃ§Ã£o do texto (implementaÃ§Ã£o bÃ¡sica).
        
        Args:
            text: Texto a ser analisado
            has_question: Se contÃ©m interrogaÃ§Ã£o
            
        Returns:
            Tupla (intent, confidence)
        """
        text_lower = text.lower()
        
        # Mapeamento bÃ¡sico de intenÃ§Ãµes
        intent_patterns = {
            'ask_price': ['quanto', 'custa', 'valor', 'preÃ§o', 'price'],
            'ask_info': ['o que', 'como', 'quando', 'onde', 'quem'],
            'request_action': ['pode', 'poderia', 'favor', 'por favor', 'faÃ§a'],
            'express_opinion': ['acho', 'penso', 'acredito', 'opiniÃ£o'],
            'express_agreement': ['concordo', 'sim', 'exato', 'certo'],
            'express_disagreement': ['discordo', 'nÃ£o', 'errado', 'incorreto']
        }
        
        for intent, patterns in intent_patterns.items():
            if any(pattern in text_lower for pattern in patterns):
                # Calcular confianÃ§a baseada em quantos padrÃµes foram encontrados
                matches = sum(1 for pattern in patterns if pattern in text_lower)
                confidence = min(0.9, 0.5 + (matches * 0.1))
                return (intent, confidence)
        
        # Default: intent genÃ©rico
        if has_question:
            return ('ask_question', 0.6)
        return ('statement', 0.5)
    
    def _detect_topic(self, text: str, keywords: List[str]) -> Tuple[str, float]:
        """
        Detecta tÃ³pico do texto (implementaÃ§Ã£o bÃ¡sica).
        
        Args:
            text: Texto a ser analisado
            keywords: Lista de keywords extraÃ­das
            
        Returns:
            Tupla (topic, confidence)
        """
        text_lower = text.lower()
        
        # Mapeamento bÃ¡sico de tÃ³picos
        topic_patterns = {
            'pricing': ['preÃ§o', 'valor', 'custo', 'price', 'quanto'],
            'product': ['produto', 'serviÃ§o', 'soluÃ§Ã£o', 'oferta'],
            'support': ['suporte', 'ajuda', 'problema', 'erro', 'bug'],
            'schedule': ['agendar', 'horÃ¡rio', 'data', 'reuniÃ£o', 'meeting'],
            'technical': ['tÃ©cnico', 'implementaÃ§Ã£o', 'cÃ³digo', 'tecnologia']
        }
        
        for topic, patterns in topic_patterns.items():
            if any(pattern in text_lower for pattern in patterns) or any(kw in patterns for kw in keywords):
                matches = sum(1 for pattern in patterns if pattern in text_lower or pattern in keywords)
                confidence = min(0.95, 0.6 + (matches * 0.1))
                return (topic, confidence)
        
        # Default: tÃ³pico genÃ©rico
        return ('general', 0.5)
    
    def _detect_speech_act(self, text: str, has_question: bool, has_exclamation: bool) -> Tuple[str, float]:
        """
        Detecta ato de fala (speech act) do texto.
        
        Args:
            text: Texto a ser analisado
            has_question: Se contÃ©m interrogaÃ§Ã£o
            has_exclamation: Se contÃ©m exclamaÃ§Ã£o
            
        Returns:
            Tupla (speech_act, confidence)
        """
        text_lower = text.lower()
        
        if has_question:
            return ('question', 0.9)
        
        if has_exclamation:
            return ('exclamation', 0.85)
        
        # Verificar padrÃµes de comandos
        command_patterns = ['favor', 'por favor', 'pode', 'poderia', 'faÃ§a', 'execute']
        if any(pattern in text_lower for pattern in command_patterns):
            return ('request', 0.8)
        
        # Verificar padrÃµes de afirmaÃ§Ã£o
        if any(word in text_lower for word in ['sim', 'certo', 'ok', 'entendi', 'concordo']):
            return ('agreement', 0.75)
        
        if any(word in text_lower for word in ['nÃ£o', 'discordo', 'errado', 'incorreto']):
            return ('disagreement', 0.75)
        
        # Default: statement
        return ('statement', 0.7)
    
    def _extract_entities(self, text: str, keywords: List[str]) -> List[str]:
        """
        Extrai entidades do texto (implementaÃ§Ã£o bÃ¡sica).
        
        Args:
            text: Texto a ser analisado
            keywords: Lista de keywords extraÃ­das
            
        Returns:
            Lista de entidades encontradas
        """
        text_lower = text.lower()
        entities = []
        
        # Entidades comuns (pode ser expandido com NER)
        entity_patterns = {
            'preÃ§o': ['preÃ§o', 'valor', 'custo', 'price'],
            'produto': ['produto', 'serviÃ§o', 'soluÃ§Ã£o'],
            'data': ['hoje', 'amanhÃ£', 'semana', 'mÃªs', 'ano'],
            'pessoa': ['vocÃª', 'eu', 'nÃ³s', 'eles']
        }
        
        for entity, patterns in entity_patterns.items():
            if any(pattern in text_lower for pattern in patterns):
                entities.append(entity)
        
        # Adicionar keywords relevantes como entidades
        for kw in keywords[:3]:  # Top 3 keywords
            if kw not in entities and len(kw) > 3:
                entities.append(kw)
        
        return entities[:5]  # Limitar a 5 entidades
    
    def _calculate_urgency(self, sentiment_score: float, has_question: bool, has_exclamation: bool, emotions: dict[str, float]) -> float:
        """
        Calcula urgÃªncia do texto (0.0 a 1.0).
        
        Args:
            sentiment_score: Score de sentimento
            has_question: Se contÃ©m interrogaÃ§Ã£o
            has_exclamation: Se contÃ©m exclamaÃ§Ã£o
            emotions: Dict de emoÃ§Ãµes
            
        Returns:
            Score de urgÃªncia (0.0 a 1.0)
        """
        urgency = 0.5  # Base
        
        # Perguntas aumentam urgÃªncia
        if has_question:
            urgency += 0.15
        
        # ExclamaÃ§Ãµes aumentam urgÃªncia
        if has_exclamation:
            urgency += 0.1
        
        # EmoÃ§Ãµes negativas aumentam urgÃªncia
        negative_emotions = emotions.get('anger', 0.0) + emotions.get('fear', 0.0)
        urgency += negative_emotions * 0.2
        
        # Sentimento negativo aumenta urgÃªncia
        if sentiment_score < 0.4:
            urgency += 0.1
        
        return min(1.0, max(0.0, urgency))

